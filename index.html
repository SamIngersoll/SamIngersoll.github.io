<!--
THESIS THESIS THESIS
CITE PAPER ABOUT HOW POPULAR TURING TEST IS
TALK MORE ABOUT HOW STRANGE MACHINE INTELLIGENCE IS
-->

<html lang="en">
<head>
  <meta charset="utf-8">
  <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="style.css">
  <script src="./js/numeric-1.2.6.js"></script>
  <script src="./js/dracula/dracula.params.js"></script>
  <script src="./js/dracula/dracula.softmax.js"></script>
  <script src="./js/dracula/dracula.tokenize.js"></script>
  <script src="./js/dracula/dracula.embeddings.js"></script>
  <script src="./js/dracula/dracula.lstm.js"></script>
  <script src="./js/dracula/dracula.params.js"></script>
  <script src="./js/dracula/dracula.js"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>
<body>
<div class="text">
  <h1>Understanding in Connectionist Models</h1>
  In recent months, some of the leading science and tech minds have warned against the creation of unregulated corporate artifical intelligence,
  but how could such an intelligence actually be created? The artificial intelligences we see today play games like Chess and Go (very well, mind you),
  recommend things for us to buy on sites like Amazon, and tag us in our friends' pictures on Facebook. These are very sophisticated machine learning models
  that can accomplish incredible feats –like identifying dog breeds solely by the sound of their bark with 94.44% accuracy
  (<a href="https://arxiv.org/pdf/1507.05546.pdf" target="_blank">Pabico et al.</a>)– but could today's neural networks really give rise to a mind?

  <h3>The Turing Test</h3>
  <figure id="xkcd">
    <a href="https://xkcd.com/329/" target="_blank"><img src="https://imgs.xkcd.com/comics/turing_test.png" width="250vw"></img>
    <figcaption>xkcd: Turing Test</figcaption></a>
  </figure>
  A good place to start when talking about artificial intelligence is with Alan Turing and the Turing Test. In his 1950 paper: <i>Computing Machinery and Intelligence</i>,
  Turing addressed the question: "can machines think?" Turing describes a test with three participants: a computer and two people. The computer and one of the
  people hold a conversation, both trying to convince the remaining person –the observer, that they are human. In the basic Turing Test, the observer would not be able to see
  either of the participants and instead reads a transcript of their dialogue (<a href="http://www.loebner.net/Prizef/TuringArticle.html" target="_blank">Turing</a>).
  <br><br>
  The Turing Test could also be used to assess speech synthesis programs by allowing the observer to hear the voices of both human and computer, or even to test androids
  by allowing the observer to use all available senses.
  <br><br>
  Turing's test was so simple and elegant that it became (and still is to this day) one of the most widely accepted tests of artificial intelligence, but some philosophers
  question whether the it can truly discern intelligence.

  <h3>The Chinese Room Thought Experiment</h3>
  In 1980, John Searle presented a thought experiment which challenges the Turing Test's ability to accurately assess intelligence. In the Chinese Room thought
  experiment, Searle argues that, given a large enough lookup table of phrases and answers, a person with no understanding of Chinese could appear to a be fluent in the language.
  This argument logically addresses the Turing Test, if the computer was equipped with a large enough lookup table, it could respond just as a human would in any scenario.
  If a really large dictionary of phrases and responses can pass the test, maybe we need a more specific test to truly evaluate intelligence!
  <br><br>
  <div align="center">Input Chinese phrase \(\Longrightarrow \) Really big lookup table \(\Longrightarrow \) Output Chinese phrase</div>
  <br>
  Searle also uses the Chinese Room argument to make the point that "computers merely use syntactic rules to manipulate symbol strings, but have no understanding of meaning or semantics"
  (<a href="https://plato.stanford.edu/entries/chinese-room/" target="_blank">Cole</a>). This is a broad argument against computational intelligence and prompted a
  number of rebuttals. Because we will be talking about neural networks, let's look at the Connectionist Reply.
  <br><br>
  The Connectionist Reply: If we imagine that our lookup table system is just one of many small parts of a vast interconnected network, we get something that looks a
  lot more like a brain. After all, the reason why we discount the Chinese Room as not <i>understanding</i> the language is that it uses a strategy that is not
  similar to our own. Searle counters the Connectionist reply by arguing that a network of small lookup table systems is still in essence the same thing as a large one,
  that the whole is not greater than the sum of its parts
  (<a href="http://www.iep.utm.edu/chineser/#SH4b" target="_blank">Hauser</a>). We will ignore Searle's dissmissing the possibility of emergent intelligence in larger networks
  and focus instead on current connectionist models. We will examine understanding of language (specifically english) and try to shed some light on whether connectionist
  models as they exist today <i>understand</i> and if they could reasonably give rise to a mind.

  <h3>The Neural Network</h3>
  Connectionism is the idea that intelligence is an emergent phenomenon, like the order in the flocking of birds, schooling of fish, or the formation of highly ordered snowflakes
  by cooling water droplets. Neural Networks are the most popular form of connectionist model. Although they are named after our brains, neural networks do not attempt to
  model the brain exactly. Like the brain, neural networks are made of a sum of many small, similar parts and much like the brain, neural networks store knowledge "in the strength of
  the connections between units. It is for this reason that this approach to understanding cognition has gained the name of connectionism"
  (<a href="https://stanford.edu/~jlmcc/papers/ThomasMcCIPCambEncy.pdf" target="_blank">Thomas and McClelland</a>). One of the most clear differences between neural networks
  and brains is the method of learning. Many neural networks use an elegant system of error minimizing that involves calculating how much each attribute of
  the network contributes to the error of the final decision using gradients and gradient descent. Some models even implement momentum to avoid local minima.
  <br><br>
  Let's talk for a moment about the anatomy of a neural network. Neural networks are made up of neurons. Neurons are split up into layers, in a fully connected network
  each neuron of a neural network has a weighted connection to each neuron in the previous layer. The first layer is called the input layer, the last layer is the output layer.
  Layers between the input and output are called hidden layers. More hidden layers allow a network to learn more abstract patterns but generally also increases training time and
  sometimes leads to overfitting: when the network learns the training data too closely and loses general predictive power.
  <figure>
    <a href="http://neuralnetworksanddeeplearning.com" target="_blank"><img src="http://neuralnetworksanddeeplearning.com/images/tikz1.png" align="center"></img>
    <figcaption>Michael Nielsen: Neural Networks and Deep Learning</figcaption></a>
  </figure>
  Similar to the network of lookup table nodes in the Connectionist Reply, each neuron performs a simple function. We can think of the neuron as a function
  depending on each of it's inputs \( y( x_{1}, x_{2}, x_{3}\ldots) \).
  <br><br>
  <figure>
    <a href="http://neuralnetworksanddeeplearning.com" target="_blank"><img src="http://neuralnetworksanddeeplearning.com/images/tikz9.png" align="center"></img>
    <figcaption>Michael Nielsen: Neural Networks and Deep Learning</figcaption></a>
  </figure>

  This is a slight simplification, but it's enough to form an intuition for how neurons function. So, how do we find the output \(y\) of some neuron \(i\)?
  First, we multiply each of the neuron's input connections by its weight \(w_{i,j}\)
  –think of these as corresponding to how much neuron \(i\) trusts input from neuron \(j\) on the previous layer, we then sum the weighted values.
  If the final value is larger than a certain threshold, the neuron "fires" or outputs a value close to one to the downstream neurons.
  If not, the neuron outputs something closer to zero –the sigmoid (\(\sigma\)) function takes care of creating this distribution.
  If we want to, we can add a bias. A bias \(b\) is just a number that changes the threshold of the neuron, making it less or more likely to fire.

  \[ y_{i} = \sigma(\sum_{j}^{} w_{i,j} x_{j} + b_{i}) \]
  <!-- \( \frac{1}{n} \sum_{i=i}^{n} x_{i} \) -->

  <h3>Connectionist Model of Cognition</h3>
  The way neural networks learn is what makes them different from the network of lookup tables in the Searle's counter. In the network of lookup tables, each node
  contains an unchanging book of values that maps inputs to ouputs. In a neural network, network inputs are completely unrecognizable in the hidden layers, the network is
  a cascade of activations and complex learned transformations on the data wich it –much like the brain– translates into something useful at the output layer.
  Below is an interface with a neural network that labels words in sentences with their corresponding parts of speech.

</div>
  <form id="input_form">
    <input type="text" name="Input" id="input" value="Join me, and together we can rule the galaxy as father and son." autocomplete="off">
    <input type="submit" id="submit">
  </form>
  <table id="output_table" align="center">
    <tbody>
      <tr>
        <td class="input">Join</td>
        <td class="input">me,</td>
        <td class="input">and</td>
        <td class="input">together</td>
        <td class="input">we</td>
        <td class="input">can</td>
        <td class="input">rule</td>
        <td class="input">the</td>
        <td class="input">galaxy</td>
        <td class="input">as</td>
        <td class="input">father</td>
        <td class="input">and</td>
        <td class="input">son.</td>
      </tr>
      <tr>
        <td class="output">Verb</td>
        <td class="output">Personal Pronoun</td>
        <td class="output">Coordinating Conj.</td>
        <td class="output">Adverb</td>
        <td class="output">Personal Pronoun</td>
        <td class="output">Verb</td>
        <td class="output">Verb</td>
        <td class="output">Determiner</td>
        <td class="output">Noun</td>
        <td class="output">Prep/Sub. Conj.</td>
        <td class="output">Noun</td>
        <td class="output">Coordinating Conj.</td>
        <td class="output">Proper Noun</td>
      </tr>
    </tbody>
  </table>
  <div align="center" id="drac-ref"><a href="https://github.com/Sentimentron/Dracula" target="_blank">Dracula</a> part-of-speech tagger by Richard Townsend</div>
  <div class="text">
    <br>
      In the example prediction, the network tags <i>son</i> as a proper noun when it's actually a plain old noun. The fact that the network was close is no accident, neural networks
      learn by recognizing patterns, they decide which output is most likely based on the patterns they've seen. This network has likely seen both <i>son</i> as a noun and
      <i>Son</i> as a proper noun in the past. It was pretty sure that <i>son</i> was a noun of some kind, but it guessed the wrong one. The fact that its incorrect guess was still
      a noun shows that it has recognized the closeness in the noun and proper noun categories and effectively gets them mixed up. It has learned a representation of
      english grammar in order to label parts of speech and has figured out the relationships between different grammatical categories in the process.
      <br><br>
      The problem with neural networks, especially deep neural networks –networks with many hidden layers– is that their inner workings get complicated quickly and it's very hard
      to extract what's going on, "not knowing why a machine did something strange leaves us unable to make sure it doesn’t happen again"
      (<a href="http://nautil.us/issue/27/dark-matter/artificial-intelligence-is-already-weirdly-inhuman" target="_blank">Berreby</a>).
      But, this complexity occurs (logically) because the network learns its own complex internal representation of the training data, which is what allows them to be such good
      general predictors (within the domain of their training data). Let's talk about the domain of the training data for a little bit.
      <h3>A Ghost in the Shell</h3>
      When you consider the richness of the inputs that our brains recieve (from all of our senses) and the depth of our neural interconnectedness, it is impressive what
      neural networks –shockingly simple by comparison, are capable of. I don't know the answer to whether neural networks could give rise to a mind, sorry!
      Maybe they are too far removed from the biological systems that inspired them, maybe the intelligence they learn is something so foreign to us that we can't recognize it.
      Maybe, just maybe there's some special threshold of richness and complexity past which the wonders of emergence allow a collection of neuron approximators to express a mind.
      <br><br>
      Maybe we're heading right towards it.
  </div>
</body>

<script src="./js/main.js"></script>
