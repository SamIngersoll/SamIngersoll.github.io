<html lang="en">
<head>
  <meta charset="utf-8">
  <link href="https://fonts.googleapis.com/css?family=Roboto" rel="stylesheet">
  <link rel="stylesheet" type="text/css" href="style.css">
  <script src="./js/numeric-1.2.6.js"></script>
  <script src="./js/dracula/dracula.params.js"></script>
  <script src="./js/dracula/dracula.softmax.js"></script>
  <script src="./js/dracula/dracula.tokenize.js"></script>
  <script src="./js/dracula/dracula.embeddings.js"></script>
  <script src="./js/dracula/dracula.lstm.js"></script>
  <script src="./js/dracula/dracula.params.js"></script>
  <script src="./js/dracula/dracula.js"></script>
  <script type="text/javascript" async
    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
</head>
<body>
<div class="text">
  <h1>Understanding in Connectionist Models</h1>
  In recent months, some of the leading science and tech minds have warned against the creation of unregulated corporate artifical intelligence,
  but how could such an intelligence actually be created? The only artificial intelligences we see today play games like Chess and Go (very well, mind you),
  recommend things for us to buy on sites like Amazon, and tag us in our friends' pictures on Facebook. These are very sophisticated machine learning models
  that can accomplish incredible feats –like identifying dog breeds solely by the sound of their bark with 94% accuracy
  (<a href="https://arxiv.org/pdf/1507.05546.pdf" target="_blank">Pabico et al.</a>)– but could they really give rise to a mind?

  <h3>The Turing Test</h3>
  <figure id="xkcd">
    <a href="https://xkcd.com/329/" target="_blank"><img src="https://imgs.xkcd.com/comics/turing_test.png" width="250vw"></img>
    <figcaption>xkcd: Turing Test</figcaption></a>
  </figure>
  A good place to start when talking about artificial intelligence is with Alan Turing and the Turing Test. In his 1950 paper: <i>Computing Machinery and Intelligence</i>,
  Turing addressed the question: "can machines think?" Turing describes a test with three participants: a computer and two people. The computer and one of the
  people hold a conversation, both trying to convince the remaining person –the observer, that they are human. In the basic Turing Test, the observer would not be able to see
  either of the participants and instead reads a transcript of their dialogue (<a href="http://www.loebner.net/Prizef/TuringArticle.html" target="_blank">Turing</a>).
  <br><br>
  The Turing Test could also be used to assess speech synthesis programs by allowing the observer to hear the voices of both human and computer, or even to test androids
  by allowing the observer to use all available senses.
  <br><br>
  Turing's test was so simple and elegant that it became (and still is to this day) one of the most widely accepted tests of artificial intelligence, but some philosophers
  question whether the it succeeds in testing intelligence at all.

  <h3>The Chinese Room Thought Experiment</h3>
  In 1980, John Searle presented a thought experiment which challenges the Turing Test's ability to accurately assess intelligence. In the Chinese Room thought
  experiment, Searle argues that, given a large enough lookup table of phrases and answers, a person with no understanding of Chinese could appear to a be fluent in the language.
  This argument logically addresses the Turing Test, if the computer was equipped with a large enough lookup table, it could respond just as a human would.
  If a really large dictionary of phrases and responses can pass the test, maybe we need a more specific test to truly evaluate intelligence!
  <br><br>
  <div align="center">Input Chinese phrase \(\Longrightarrow \) Really big lookup table \(\Longrightarrow \) Output Chinese phrase</div>
  <br>
  Searle also uses the Chinese Room argument to make the point that "computers merely use syntactic rules to manipulate symbol strings, but have no understanding of meaning or semantics"
  (<a href="https://plato.stanford.edu/entries/chinese-room/" target="_blank">Cole</a>). This is a broad argument against computational intelligence and prompted a
  number of rebuttals. Because we will be talking about connectionist models, let's look at the Connectionist Reply.
  <br><br>
  The Connectionist Reply: If we imagine that our lookup table system is just one of many small parts of a vast interconnected network, we get something that looks a
  lot more like a brain. After all, the reason why we discount the Chinese Room as not <i>understanding</i> the language is that it uses a strategy that is not
  similar to our own. Searle counters the Connectionist reply by arguing that a network of small lookup table systems is still in essece the same thing as a large one,
  that the whole is not greater than the sum of its parts
  (<a href="http://www.iep.utm.edu/chineser/#SH4b" target="_blank">Hauser</a>). We will ignore Searle's dissmissing the possibility of emergent intelligence in larger networks
  and focus instead on current connectionist models. We will examine understanding of language (specifically english) and try to shed some light on whether connectionist
  models as they exist today <i>understand</i> and if they could reasonably give rise to a mind.

  <h3>The Neural Network</h3>
  Neural Networks are the most popular form of connectionist model. Although they are named after our brains, neural networks do not attempt to model the brain.
  Like the brain, neural networks are made of a sum of small, similar parts. Like the brain, neural networks store knowledge "in the strength of the connections between units.
  It is for this reason that this approach to understanding cognition has gained the name of connectionism"
  (<a href="https://stanford.edu/~jlmcc/papers/ThomasMcCIPCambEncy.pdf" target="_blank">Thomas and McClelland</a>, 2). This is where the similarities end for the most part.
  <br><br>
  Let's talk for a moment about the anatomy of a neural network. Neural networks are made up of neurons. Neurons are split up into layers, in a fully connected network
  each neuron of a neural network has a weighted connection to each neuron in the previous layer. The first layer is called the input layer, the last layer is the output layer.
  Layers between the input and output are called hidden layers. More hidden layers allow a network to learn more abstract patterns but generally also increase training time and
  sometimes leads to overfitting: when the network learns the training data too closely and its general predictive power decreases.
  <figure>
    <a href="http://neuralnetworksanddeeplearning.com" target="_blank"><img src="http://neuralnetworksanddeeplearning.com/images/tikz1.png" align="center"></img>
    <figcaption>Michael Nielsen: Neural Networks and Deep Learning</figcaption></a>
  </figure>
  Similar to the network of lookup table nodes in the Connectionist Reply, each neuron performs a simple function. We can think of the neuron as a function
  depending on each of it's inputs \( y( x_{1}, x_{2}, x_{3}\ldots) \).
  <br><br>
  <figure>
    <a href="http://neuralnetworksanddeeplearning.com" target="_blank"><img src="http://neuralnetworksanddeeplearning.com/images/tikz9.png" align="center"></img>
    <figcaption>Michael Nielsen: Neural Networks and Deep Learning</figcaption></a>
  </figure>

  This is a slight simplification, but it's enough to form an intuition of how a nueron functions. So, how do we find the output \(y\) of some neuron \(i\).
  First, we multiply each of the neuron's input connections by its weight \(w_{i,j}\)
  –think of these as corresponding to how much neuron \(i\) trusts input from neuron \(j\) on the previous layer, we then sum the weighted values.
  If the final value is larger than a certain threshold, the neuron "fires" or outputs a value close to one to the downstream neurons.
  If not, the neuron outputs something closer to zero –the sigmoid (\(\sigma\)) function takes care of this.
  If we want to, we can add a bias. A bias \(b\) is just a number that changes the threshold of the neuron, making it less or more likely to fire.

  \[ y_{i} = \sigma(\sum_{j}^{} w_{i,j} x_{j} + b_{i}) \]
  <!-- \( \frac{1}{n} \sum_{i=i}^{n} x_{i} \) -->

  <h3>Connectionist Model of Cognition</h3>
  The way neural networks learn is what makes them differen from the network of lookup tables in the Searle's counter. In the network of lookup tables, each node
  contains an unchanging book of values that maps inputs to ouputs. In a neural network, the knowledge stored in the connections may not map logically to the output
  of the network. In the input and output layers, the outputs of the neurons will make sense, but in the hidden layers the network learns abstract patterns. It abstracts
  its inputs and transforms them in strange ways that the programmer does not know about, it no longer deals with the inputs, but with cascading activations and complex learned
  transformations on the data, much like the brain. Below is a neural network that labels words in a sentence with which part of
  speech they are.

</div>
  <form id="input_form">
    <input type="text" name="Input" id="input" value="Join me, and together we can rule the galaxy as father and son." autocomplete="off">
    <input type="submit" id="submit">
  </form>
  <table id="output_table" align="center">
    <tbody>
      <tr>
        <td class="input">Join</td>
        <td class="input">me,</td>
        <td class="input">and</td>
        <td class="input">together</td>
        <td class="input">we</td>
        <td class="input">can</td>
        <td class="input">rule</td>
        <td class="input">the</td>
        <td class="input">galaxy</td>
        <td class="input">as</td>
        <td class="input">father</td>
        <td class="input">and</td>
        <td class="input">son.</td>
      </tr>
      <tr>
        <td class="output">Verb</td>
        <td class="output">Personal Pronoun</td>
        <td class="output">Coordinating Conj.</td>
        <td class="output">Adverb</td>
        <td class="output">Personal Pronoun</td>
        <td class="output">Verb</td>
        <td class="output">Verb</td>
        <td class="output">Determiner</td>
        <td class="output">Noun</td>
        <td class="output">Prep/Sub. Conj.</td>
        <td class="output">Noun</td>
        <td class="output">Coordinating Conj.</td>
        <td class="output">Proper Noun</td>
      </tr>
    </tbody>
  </table>
  <div align="center" id="drac-ref"><a href="https://github.com/Sentimentron/Dracula" target="_blank">Dracula</a> part-of-speech tagger by Richard Townsend</div>
  <div class="text">
    <br>
      In the example prediction, the network tags <i>son</i> as a proper noun when it's just a normal noun. The fact that the network was close is no accident. Neural networks
      learn by recognizing patterns, they decide which output is most likely based on the patterns they've seen. This network has likely seen both <i>son</i> as a noun and
      <i>Son</i> as a proper noun in the past. It was pretty sure that <i>son</i> was a noun of some kind, but it guessed the wrong one.
      <br><br>
      The problem with neural networks, especially deep neural networks –networks with many hidden layers– is that their inner workings get complicated quickly and it's very hard
      to extract what's going on. But, this happens because the network learns a complex internal representation of the training data, which is what allows them to be such good
      general predictors (within the domain of their training data). Becuase of their impenetrable nature, trained neural networks are often seen as <i>black boxes:</i>
      systems that we deal with using their inputs and outputs rather than their internal workings, much like the human brain.
  </div>
</body>

<script src="./js/main.js"></script>
